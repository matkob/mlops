{
  "pipelineSpec": {
    "components": {
      "comp-create-dataset": {
        "executorLabel": "exec-create-dataset",
        "outputDefinitions": {
          "artifacts": {
            "output_csv": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-split-dataset": {
        "executorLabel": "exec-split-dataset",
        "inputDefinitions": {
          "artifacts": {
            "input_csv": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "seed": {
              "type": "INT"
            },
            "test_split": {
              "type": "DOUBLE"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "x_test_csv": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "x_train_csv": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "y_test_csv": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "y_train_csv": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-train-and-test-model": {
        "executorLabel": "exec-train-and-test-model",
        "inputDefinitions": {
          "artifacts": {
            "x_test_csv": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "x_train_csv": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "y_test_csv": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            },
            "y_train_csv": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "model_file": {
              "artifactType": {
                "schemaTitle": "system.Model",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      },
      "comp-transform-dataset": {
        "executorLabel": "exec-transform-dataset",
        "inputDefinitions": {
          "artifacts": {
            "input_csv": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          },
          "parameters": {
            "time_window": {
              "type": "INT"
            }
          }
        },
        "outputDefinitions": {
          "artifacts": {
            "output_csv": {
              "artifactType": {
                "schemaTitle": "system.Dataset",
                "schemaVersion": "0.0.1"
              }
            }
          }
        }
      }
    },
    "deploymentSpec": {
      "executors": {
        "exec-create-dataset": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "create_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'kfp==1.8.19' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef create_dataset(output_csv: dsl.Output[dsl.Dataset]):\n    \"\"\"\n    Fetches dataset from tardis.dev. Must be of type: order book snapshot.\n    \"\"\"\n    import requests\n\n    url = \"https://datasets.tardis.dev/v1/deribit/book_snapshot_25/2020/04/01/BTC-PERPETUAL.csv.gz\"\n    with open(output_csv.path, \"wb\") as output_file:\n        output_file.write(requests.get(url).content)\n\n"
            ],
            "image": "python:3.10"
          }
        },
        "exec-split-dataset": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "split_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'polars~=0.16.9' 'scikit-learn~=1.0.2' 'kfp==1.8.19' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef split_dataset(input_csv: dsl.Input[dsl.Dataset], test_split: float, seed: int, x_train_csv: dsl.Output[dsl.Dataset],\n                  y_train_csv: dsl.Output[dsl.Dataset], x_test_csv: dsl.Output[dsl.Dataset],\n                  y_test_csv: dsl.Output[dsl.Dataset]):\n    import polars as pl\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n\n    df = pl.read_csv(input_csv.path, sep=\",\")\n    y = df[\"mid.price\"].to_numpy()\n    X = df.drop(columns=\"mid.price\").to_numpy()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_split, random_state=seed)\n\n    np.savetxt(x_train_csv.path, X_train, delimiter=\",\")\n    np.savetxt(y_train_csv.path, y_train, delimiter=\",\")\n    np.savetxt(x_test_csv.path, X_test, delimiter=\",\")\n    np.savetxt(y_test_csv.path, y_test, delimiter=\",\")\n\n"
            ],
            "image": "python:3.10"
          }
        },
        "exec-train-and-test-model": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "train_and_test_model"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'xgboost~=1.6.2' 'scikit-learn~=1.0.2' 'kfp==1.8.19' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef train_and_test_model(x_train_csv: dsl.Input[dsl.Dataset], y_train_csv: dsl.Input[dsl.Dataset],\n                         x_test_csv: dsl.Input[dsl.Dataset], y_test_csv: dsl.Input[dsl.Dataset],\n                         model_file: dsl.Output[dsl.Model]):\n    import numpy as np\n    import xgboost as xgb\n    import sklearn.metrics as metrics\n\n    X_train = np.genfromtxt(x_train_csv.path, delimiter=\",\")\n    y_train = np.genfromtxt(y_train_csv.path, delimiter=\",\")\n    X_test = np.genfromtxt(x_test_csv.path, delimiter=\",\")\n    y_test = np.genfromtxt(y_test_csv.path, delimiter=\",\")\n\n    model = xgb.XGBRegressor()\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n\n    max_error = metrics.max_error(y_test, predictions)\n    median_error = metrics.median_absolute_error(y_test, predictions)\n    mse_error = metrics.mean_squared_error(y_test, predictions)\n    explained_variance = metrics.explained_variance_score(y_test, predictions)\n\n    model.save_model(model_file.path)\n    model_file.metadata[\"Max error\"] = max_error\n    model_file.metadata[\"Median error\"] = median_error\n    model_file.metadata[\"MSE\"] = mse_error\n    model_file.metadata[\"Explained variance\"] = explained_variance\n\n"
            ],
            "image": "python:3.10"
          }
        },
        "exec-transform-dataset": {
          "container": {
            "args": [
              "--executor_input",
              "{{$}}",
              "--function_to_execute",
              "transform_dataset"
            ],
            "command": [
              "sh",
              "-c",
              "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location 'polars~=0.16.9' 'kfp==1.8.19' && \"$0\" \"$@\"\n",
              "sh",
              "-ec",
              "program_path=$(mktemp -d)\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
              "\nimport kfp\nfrom kfp.v2 import dsl\nfrom kfp.v2.dsl import *\nfrom typing import *\n\ndef transform_dataset(input_csv: dsl.Input[dsl.Dataset], time_window: int, output_csv: dsl.Output[dsl.Dataset]):\n    import polars as pl\n\n    # TODO: increase memory resource limits and remove nrows limit\n    df = pl.read_csv(input_csv.path, n_rows=500, sep=\",\", infer_schema_length=0)\n    df = df.with_columns(df[\"timestamp\"].cast(pl.Int64))\n    # using exchange timestamp\n    df = df.sort(\"timestamp\")\n    # exchange, symbol is always binance, btcusdt\n    df = df.drop(columns=[\"timestamp\", \"local_timestamp\", \"exchange\", \"symbol\"])\n    # making sure there are only asks and bids in the dataset\n    asks_n_bids = [c for c in df.columns if \"asks\" in c or \"bids\" in c]\n    df = df[asks_n_bids].with_columns(pl.all().cast(pl.Float32))\n    # adding target - mid price\n    df = df.with_columns(((df[\"asks[0].price\"] + df[\"bids[0].price\"]) / 2).alias(\"mid.price\"))\n    # shifting input data\n    for i in range(1, time_window + 1):\n        cols = [f\"{i}_tick_{c}\" for c in asks_n_bids]\n        shifted = df[asks_n_bids].shift(periods=i)\n        shifted.columns = cols\n        df = df.with_columns(shifted)\n    # removing data leakage\n    df = df.drop(columns=asks_n_bids)\n    # removing NaNs\n    df = df.drop_nulls()\n\n    df.write_csv(output_csv.path)\n\n"
            ],
            "image": "python:3.10"
          }
        }
      }
    },
    "pipelineInfo": {
      "name": "dummy-regression-model"
    },
    "root": {
      "dag": {
        "tasks": {
          "create-dataset": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-create-dataset"
            },
            "taskInfo": {
              "name": "create-dataset"
            }
          },
          "split-dataset": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-split-dataset"
            },
            "dependentTasks": [
              "transform-dataset"
            ],
            "inputs": {
              "artifacts": {
                "input_csv": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "output_csv",
                    "producerTask": "transform-dataset"
                  }
                }
              },
              "parameters": {
                "seed": {
                  "componentInputParameter": "seed"
                },
                "test_split": {
                  "componentInputParameter": "test_split"
                }
              }
            },
            "taskInfo": {
              "name": "split-dataset"
            }
          },
          "train-and-test-model": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-train-and-test-model"
            },
            "dependentTasks": [
              "split-dataset"
            ],
            "inputs": {
              "artifacts": {
                "x_test_csv": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "x_test_csv",
                    "producerTask": "split-dataset"
                  }
                },
                "x_train_csv": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "x_train_csv",
                    "producerTask": "split-dataset"
                  }
                },
                "y_test_csv": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "y_test_csv",
                    "producerTask": "split-dataset"
                  }
                },
                "y_train_csv": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "y_train_csv",
                    "producerTask": "split-dataset"
                  }
                }
              }
            },
            "taskInfo": {
              "name": "train-and-test-model"
            }
          },
          "transform-dataset": {
            "cachingOptions": {
              "enableCache": true
            },
            "componentRef": {
              "name": "comp-transform-dataset"
            },
            "dependentTasks": [
              "create-dataset"
            ],
            "inputs": {
              "artifacts": {
                "input_csv": {
                  "taskOutputArtifact": {
                    "outputArtifactKey": "output_csv",
                    "producerTask": "create-dataset"
                  }
                }
              },
              "parameters": {
                "time_window": {
                  "componentInputParameter": "time_window"
                }
              }
            },
            "taskInfo": {
              "name": "transform-dataset"
            }
          }
        }
      },
      "inputDefinitions": {
        "parameters": {
          "seed": {
            "type": "INT"
          },
          "test_split": {
            "type": "DOUBLE"
          },
          "time_window": {
            "type": "INT"
          }
        }
      }
    },
    "schemaVersion": "2.0.0",
    "sdkVersion": "kfp-1.8.19"
  },
  "runtimeConfig": {
    "gcsOutputDirectory": "gs://test-vertex-ai-datasets/kubeflow",
    "parameters": {
      "seed": {
        "intValue": "2"
      },
      "test_split": {
        "doubleValue": 0.33
      },
      "time_window": {
        "intValue": "2"
      }
    }
  }
}